# -*- coding: utf-8 -*-
"""topic_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ADKBcEDE0pwt8FjtMYEgcvQpd13_wIZA
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers
# !pip install sentencepiece

"""**IMPORTING**"""

#importing the necessary libraries

import re
import spacy
import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import torch

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from tqdm.notebook import tqdm
from transformers import AutoTokenizer, BertTokenizer, DistilBertTokenizer
from torch.utils.data import TensorDataset
from transformers  import BertForSequenceClassification, AutoModel, AutoModelForSequenceClassification, DistilBertForSequenceClassification
from transformers import AdamW, get_linear_schedule_with_warmup
from transformers import TextClassificationPipeline

nlp = spacy.load("en_core_web_sm")

#df = pd.read_csv("/content/train.csv")
#df.shape

"""**TEXT PREPROCESSING**"""

#Reading in the training and testing files
train_df = pd.read_csv("/content/drive/MyDrive/Otherweb/Dataset versions/final_topic_gt_100_plus_adjustment_dataset.csv")
train_df.head()

train_df.shape

#renaming the columns
train_df.rename(columns={"article_text": "Text", "topic": "Category"}, inplace=True)

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased",
                                                do_lower_case=True
                                                )

def chunker(text, tokenizer, sep='\n', max_len=512):
    """Split long text into chunks"""
    # Split into sentences
    sentences = []
    spacy_doc = nlp(text)

    for sent in spacy_doc.sents:
      sentences.append(str(sent))
    # Drop empty sentences
    sentences = [s for s in sentences if len(s.strip())]
    # Compute token lengths
    lens = [len(t) for t in tokenizer(sentences, return_attention_mask=False, return_tensors="pt", truncation=True, padding=True)['input_ids']]

    chunks, clen = [[]], 0
    for i, l in enumerate(lens):
        if clen+l <= max_len:
            chunks[-1].append(i)
            clen += l
        else:
            chunks.append([i])
            clen = l

    paragraphs = [sep.join(sentences[i] for i in chunk) for chunk in chunks]
    return paragraphs

new_data = pd.DataFrame(columns=["Text", "Category"])

for i in range(train_df.shape[0]):
  article = train_df.iloc[i]["Text"]
  label = train_df.iloc[i]["Category"]

  chunks = chunker(article, tokenizer)
  #chunks = chunks[1:]

  for chunk in chunks:
    new_data = new_data.append({"Text":chunk, "Category":label}, ignore_index=True)

train_df = new_data

train_df.shape

#Dropping null values
train_df = train_df.dropna(axis=0)

from google.colab import drive
drive.mount("/content/drive")

train_df['Category'].value_counts()

train_df = train_df.sample(frac=1).reset_index(drop=True)

#Getting all unique classes from the training_set
possible_labels = train_df["Category"].unique()

#Generating a dict where every class map to an id
label_dict = {}
for index, possible_label in enumerate(possible_labels):
  label_dict[possible_label] = index

#Adding the numerical labels into the datafram
train_df["label"] = train_df["Category"].replace(label_dict)

#Distribution of training data per class

train_df["Category"].value_counts(ascending=True).plot.barh()
plt.title("Category Count")

#Get the percentage of the whole class in the training set

category_percentage = (100*train_df.Category.value_counts()/len(train_df)).to_frame().reset_index()
category_percentage.columns = ["category", "percentage"]

#visualize the output
plt.figure(figsize=(15,8))
chart = sns.barplot(x="category", y="percentage", data=category_percentage)
chart.set_title("percantage of each class")
chart.set_xticklabels(chart.get_xticklabels(), rotation=30)

for p in chart.patches:
  chart.annotate("%0.2f"% p.get_height(), (p.get_x() + p.get_width()/2., p.get_height()),
                 ha='center', va='center', fontsize=10, color='black',xytext=(0,5),
                 textcoords= "offset points")

#Getting to know the number of tokens in each class
train_df['#tokens'] = train_df['Text'].apply(lambda x: len(x.split()))

#Let us visaulize the average number of tokens for each class

avg_tokens_per_cat = train_df.groupby('Category')['#tokens'].mean().to_frame().reset_index()
#visualize the result
plt.figure(figsize=(15,8))
chart = sns.barplot(x="Category", y="#tokens",data = avg_tokens_per_cat)
chart.set_title("Average token per class")
chart.set_xticklabels(chart.get_xticklabels(), rotation=30)

for p in chart.patches:
  chart.annotate("%0.2f" % p.get_height(), (p.get_x() + p.get_width()/ 2., p.get_height()),
                 ha="center", va='center', fontsize=10, color='black',xytext=(0,5),
                 textcoords = "offset points")

#Generating the maxmimum tokens per each class

max_tokens_per_cat = train_df.groupby('Category')['#tokens'].max().to_frame().reset_index()
#visualize result
plt.figure(figsize=(15,8))
chart = sns.barplot(x='Category', y='#tokens',data=max_tokens_per_cat)
chart.set_title("Max tokens per category")
chart.set_xticklabels(chart.get_xticklabels(), rotation=30)

for p in chart.patches:
  chart.annotate("%0.2f" % p.get_height(), (p.get_x() + p.get_width()/2., p.get_height()),
                  ha='center', va="center", fontsize=10, color="black",xytext=(0,5),
                  textcoords = "offset points")

#Distribution of words per text

train_df['Words per news text'] = train_df['Text'].str.split().apply(len)
train_df.boxplot('Words per news text', by='Category', grid=False, showfliers=False, color='black')
plt.suptitle("")
plt.xlabel("")

train_df.head()

#splitting the dataset into training and testing set
x_train, x_val, y_train, y_val = train_test_split(train_df.index.values,
                                                  train_df.label.values,
                                                  test_size=0.1,
                                                  random_state=42,
                                                  stratify = train_df.label.values
                                                  )

#specify the datatype

train_df['data_type'] = ['not_set']*train_df.shape[0]
train_df.loc[x_train, 'data_type'] = 'train'
train_df.loc[x_val, 'data_type'] = 'val'

train_df.groupby(['Category', 'label', 'data_type']).count()

"""**MODELLING**"""

#labels = ["Politics", "Business", "Health", "Sports", "Tech", "World", "Science", "Entertainment"]

#initialize a distillbert tokenizer

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased",
                                                do_lower_case=True
                                                )

#Encode the training dataset
encoded_data_train = tokenizer.batch_encode_plus(
    train_df[train_df.data_type == 'train'].Text.values,
    add_special_tokens=True,
    return_attention_mask = True,
    padding='max_length',
    truncation = True,
    max_length = 512,
    return_tensors = 'pt',)

#encoding the val dataset
encoded_data_val = tokenizer.batch_encode_plus(
    train_df[train_df.data_type == 'val'].Text.values,
    add_special_tokens=True,
    return_attention_mask =True,
    padding = 'max_length',
    truncation = True,
    max_length = 512,
    return_tensors = 'pt'
)

input_ids_train = encoded_data_train['input_ids']
attention_mask_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(train_df[train_df.data_type == 'train'].label.values)

input_ids_val = encoded_data_val['input_ids']
attention_mask_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(train_df[train_df.data_type == 'val'].label.values)

#intansiate a distilbert model

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                                            num_labels = len(label_dict),
                                                            output_attentions=False,
                                                            output_hidden_states = False)

#Prepare tensor datasets

dataset_train = TensorDataset(input_ids_train, attention_mask_train, labels_train)
dataset_val = TensorDataset(input_ids_val, attention_mask_val, labels_val)

batch_size = 3

dataloader_train = DataLoader(dataset_train,
                              sampler = RandomSampler(dataset_train),
                              batch_size = batch_size)

dataloader_validation = DataLoader(dataset_val,
                             sampler = RandomSampler(dataset_val),
                             batch_size = batch_size)

optimizer = AdamW(model.parameters(),
                  lr=1e-5,
                  eps=1e-8)

epochs = 10
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

#Getting the f1_score of the class
def f1_score_func(preds, labels):
  preds_flat = np.argmax(preds, axis=1).flatten()
  labels_flat = labels.flatten()
  return f1_score(labels_flat, preds_flat, average='weighted')

#Getting the accuracy per each class
def accuracy_per_class(preds, labels):
  label_dict_inverse = {v:k for k, v in label_dict.items()}
  preds_flat = np.argmax(preds, axis=1).flatten()
  labels_flat = labels.flatten()

  for label in np.unique(labels_flat):
    y_preds = preds_flat[labels_flat == label]
    y_true = labels_flat[labels_flat == label]
    print(f'Class: {label_dict_inverse[label]}')

    print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\n')

#seeding all

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

#connecting to gpu
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(device)

def evaluate(dataloader_val):
  model.eval()

  loss_val_total = 0
  predictions, true_vals = [], []

  for batch in dataloader_val:
    batch = tuple(b.to(device) for b in batch)

    inputs = {'input_ids': batch[0],
              'attention_mask': batch[1],
              'labels': batch[2]
              }
    with torch.no_grad():
      outputs = model(**inputs)

    loss = outputs[0]
    logits = outputs[1]
    loss_val_total += loss.item()

    logits = logits.detach().cpu().numpy()
    labels_ids = inputs['labels'].cpu().numpy()
    predictions.append(logits)
    true_vals.append(labels_ids)

  loss_val_avg = loss_val_total/len(dataloader_val)
  predictions  = np.concatenate(predictions, axis=0)
  true_vals = np.concatenate(true_vals, axis=0)

  return loss_val_avg, predictions, true_vals

for epoch in tqdm(range(1, epochs +1)):
  model.train()

  loss_train_total = 0
  progress_bar = tqdm(dataloader_train, desc = 'Epoch {:1d}'.format(epoch), leave=False, disable=False)

  for batch in progress_bar:
    model.zero_grad()

    batch =  tuple(b.to(device) for b in batch)

    inputs = {'input_ids': batch[0],
              'attention_mask': batch[1],
              'labels': batch[2]
        }

    outputs = model(**inputs)

    loss = outputs[0]
    loss_train_total += loss.item()
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    optimizer.step()
    scheduler.step()

    progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})

  torch.save(model.state_dict(), f'finetuned_distilBERT_epoch_{epoch}.model')

  tqdm.write(f'\nEpoch {epoch} ')

  loss_train_avg = loss_train_total/len(dataloader_train)

  tqdm.write(f'Training loss: {loss_train_avg}')

  val_loss, predictions, true_vals = evaluate(dataloader_validation)
  val_f1 = f1_score_func(predictions, true_vals)
  tqdm.write(f'Validation loss: {val_loss}')
  tqdm.write(f'F1 score(weighted): {val_f1}')

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',
                                            num_labels = len(label_dict),
                                            output_attentions = False,
                                            output_hidden_states = False
                                            )

model.to(device)

model.load_state_dict(torch.load('/content/finetuned_distilBERT_epoch_9.model', map_location = torch.device('cpu')))
model.save_pretrained("finetuned_distilbert_news_article_categorization_model")
tokenizer.save_pretrained('finetuned_distilert_news_article_categorization_tokenizer')

_, predictions, true_vals = evaluate(dataloader_validation)
print(accuracy_per_class(predictions, true_vals))

"""**MODEL EVALUATION**"""

#Let us predict on an unseen news
#abels = ['Entertainment', 'World','health', 'Politics', 'sport', 'Tech','Business', 'Science']
custom_news  = """CompaniesJOHANNESBURG, Dec 12 (Reuters) - Aspen Pharmacare (APNJ.J) has wrapped up pacts to secure funding of $30 million from the Gates Foundation and the Coalition for Epidemic Preparedness Innovations to help make affordable vaccines for Africa, the company said on Monday.In August, Aspen announced a deal with the Serum Institute of India to make and sell four Aspen-branded vaccines for Africa, as it looks to use its near-idle COVID-19 vaccine production lines in South Africa.It had been negotiating with the two organizations for grant funding.\"The new funding from CEPI and the Gates Foundation will support a ten-year agreement between Aspen and Serum Institute that aims to expand the supply and sourcing of affordable vaccines manufactured in Africa,\" the company said.[1/2]Â People wearing face masks walk past a logo of South African pharmaceutical major Aspen Pharmacare, at its Johnson & Johnson COVID-19 vaccine facility in Gqeberha, South Africa, October 25, 2021. REUTERS/Siphiwe SibekoEach of the two will contribute $15 million.The Serum Institute agreement provides for Aspen to make and distribute pneumococcal, rotavirus, polyvalent meningococcal and hexavalent vaccines commonly administered in Africa.It gives Aspen certainty on volumes, which will eventually more than cover an expected fall in revenue from its COVID vaccine contract with Johnson & Johnson (JNJ.N), Chief Executive Stephen Saad said in August.On Monday, Aspen said the funding, in addition to supporting the \"technology transfer activities\" set to begin early in 2023, will help sustain its regional vaccine making capacity against future outbreaks.Reporting by Nqobile Dludla; Editing by Louise Heavens and Clarence FernandezOur Standards: The Thomson Reuters Trust Principles.
"""
input_tensor = tokenizer.encode(custom_news, return_tensors='pt', truncation=True).to('cuda')
logits = model(input_tensor).logits
logits

#compute probability from logits classification model outputs
softmax = torch.nn.Softmax(dim=1)
probs = softmax(logits)[0]
probs = probs.cpu().detach().numpy()
probs

#visualizing the probability of each class

plt.bar(labels, 100*probs, color='C0')
plt.title('Prediction for the custom text')
plt.ylabel("Class probability (%)")

#connecting to my hugging face account
#hide_output
from huggingface_hub import notebook_login

notebook_login()

#loading the model and tokenizer to valurank hugging face space
model.push_to_hub('finetuned-distilbert-topic_classification')

