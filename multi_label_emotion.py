# -*- coding: utf-8 -*-
"""multi_label_emotion_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bITy4mq6tZnRwHhUYA35Q5rpC6oIXPSU
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from sklearn import metrics
from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import random
import os
import torch
import warnings

warnings.filterwarnings('ignore')

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f'Using GPU : {torch.cuda.get_device_name(0)}')
else:
    device = torch.device("cpu")
    print(f'Using CPU')

#read in the dataset
data = pd.read_excel("/content/drive/MyDrive/balanced_labeled_emotion_headline_2_29000.xlsx")
#df_2 = pd.read_excel("/content/drive/MyDrive/labeled_emotion_headline_4_15000.xlsx")
#df_3 = pd.read_excel("/content/drive/MyDrive/labeled_emotion_headline_3_5000.xlsx")
#new_df = pd.read_excel("/content/drive/MyDrive/labeled_emotion_new_df.xlsx")

new_df.to_excel("/content/drive/MyDrive/labeled_emotion_new_df.xlsx", index=False)

data = pd.concat([df_1, df_2, df_3])

data = data.sample(frac = 1)

data.isna().sum()

#dropping the Envious column
data = data.drop(columns=["Envious"], axis=1)

data.head()

new = data[data['count'] == 1]
new.head()

data = data.drop(data[data['Dismissive'] == 1].sample(frac=.05).index)

["Cute", "Infuriating", "Sentimental", "Empathetic",
               "Cynical", "Depressing", "Awe-inspiring", "Patriotic", "Educational",
               "Encouraging", "Voyeuristic", "Funny", "Sarcastic", "Dismissive", "Disparaging"]

data["Infuriating"].value_counts()

data.head()

new_df = pd.DataFrame()
new_df['text'] = data['Headline']
new_df['labels'] = data.iloc[:, 1:].values.tolist()

def count_ones(text):
  return text.count(1)


new_df["count"] = new_df["labels"].apply(lambda x: count_ones(x))

new_df["count"].value_counts()

new_df = new_df.loc[new_df['count'] > 0]

new_df.head()

text_data = new_df.text.values
labels_data = list(new_df.labels)

model_name = 'distilbert-base-uncased'

tokenizer = DistilBertTokenizer.from_pretrained(model_name)

max_len = np.zeros(len(text_data))
for i in range(len(text_data)):
    input_ids = tokenizer.encode(text_data[i], add_special_tokens=True)
    max_len[i] = len(input_ids)
print('Max length: ', max_len.max())

input_ids = []
attention_masks = []

for text in text_data:
    encoded_dict = tokenizer.encode_plus(
                        text,
                        add_special_tokens = True,
                        max_length = 128,
                        pad_to_max_length = True,
                        truncation=True,
                        return_attention_mask = True,
                        return_tensors = 'pt')
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim=0)
attention_masks = torch.cat(attention_masks, dim=0)
labels_tensor = torch.tensor(labels_data)

dataset = TensorDataset(input_ids, attention_masks, labels_tensor)
train_size = int(0.9 * len(dataset))
val_size = int(0.05 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, test_dataset = random_split(dataset, [train_size + val_size, test_size])
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

print(f'Total amount of samples : {train_size + val_size + test_size}')
print(f'Train set : {train_size}')
print(f'Validation set : {val_size}')
print(f'Test set : {test_size}')

batch_size = 16

train_dataloader = DataLoader(
            train_dataset,
            sampler = RandomSampler(train_dataset),
            batch_size = batch_size)

validation_dataloader = DataLoader(
            val_dataset,
            sampler = RandomSampler(val_dataset),
            batch_size = batch_size)

test_dataloader = DataLoader(
            test_dataset,
            sampler = RandomSampler(test_dataset),
            batch_size = batch_size)

model = DistilBertForSequenceClassification.from_pretrained(
    model_name,
    problem_type="multi_label_classification",
    num_labels = 13,
    output_attentions = False,
    output_hidden_states = False,
)

model.cuda()

learning_rate = 2e-5

optimizer = AdamW(model.parameters(),
                  lr = learning_rate)

epochs = 5

total_steps = len(train_dataloader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

def get_probs(logits, threshold=0.5):
    sigm = 1 / (1 + np.exp(-logits))
    return sigm

def flat_accuracy(preds, labels):
    res = np.zeros(labels.shape[0])
    for i in range(labels.shape[0]):
        res[i] = np.all(preds[i] == labels[i])
    return np.sum(res) / labels.shape[0]

def compute_f1_macro(out, pred):
    return metrics.f1_score(pred, out, average='macro')

def compute_f1_micro(out, pred):
    return metrics.f1_score(pred, out, average='micro')

print('Training started...')

np.random.seed(42)
random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

training_stats = []

for epoch_i in range(epochs):
    print()
    print('#-----------------------#')
    print(f'     Epoch : {epoch_i + 1} / {epochs}')
    print('#-----------------------#')

    model.train()
    total_train_loss = 0

    for step, batch in enumerate(train_dataloader):
        batch_input_ids = batch[0].to(device)
        batch_input_mask = batch[1].to(device)
        batch_labels = batch[2].float().to(device)

        model.zero_grad()

        result = model(batch_input_ids,
                        attention_mask=batch_input_mask,
                        labels=batch_labels,
                        return_dict=True)
        loss = result.loss
        logits = result.logits

        total_train_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)

    print(f'Average train loss : {avg_train_loss:.3f}')
    print()
    print('Validation started...')
    print()

    model.eval()

    total_eval_accuracy = 0
    total_eval_loss = 0
    total_eval_f1_micro = 0
    total_eval_f1_macro = 0
    nb_eval_steps = 0

    for batch in validation_dataloader:
        batch_input_ids = batch[0].to(device)
        batch_input_mask = batch[1].to(device)
        batch_labels = batch[2].float().to(device)

        with torch.no_grad():
            result = model(batch_input_ids,
                            attention_mask=batch_input_mask,
                            labels=batch_labels,
                            return_dict=True)

        loss = result.loss
        logits = result.logits

        total_eval_loss += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = batch_labels.to('cpu').numpy()

        total_eval_f1_micro += compute_f1_micro(get_probs(logits), label_ids)
        total_eval_f1_macro += compute_f1_macro(get_probs(logits), label_ids)
        total_eval_accuracy += flat_accuracy(get_probs(logits), label_ids)

    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    avg_val_f1_micro = total_eval_f1_micro / len(validation_dataloader)
    avg_val_f1_macro = total_eval_f1_macro / len(validation_dataloader)
    avg_val_loss = total_eval_loss / len(validation_dataloader)

    print(f'Average validation loss : {avg_val_loss:.3f}')
    print('Average validation metrics:')
    print('----------------')
    print(f'Accuracy : {avg_val_accuracy:.3f}')
    print(f'f1-score micro : {avg_val_f1_micro:.3f}')
    print(f'f1-score macro : {avg_val_f1_macro:.3f}')

    training_stats.append(
            {
                'epoch': epoch_i + 1,
                'train_loss': avg_train_loss,
                'valid_loss': avg_val_loss,
                'val_accuracy': avg_val_accuracy,
                'val_f1_micro' : avg_val_f1_micro,
                'val_f1_macro' : avg_val_f1_macro
})

print()
print('Training finished...')

text = """Mayra Flores Wins Texasâ€™s 34th District Special Election: Full Results"""

model.eval()
#print(f'Commit message: \n{r')
inputs = tokenizer(text, return_tensors="pt", truncation=True)
model.to("cpu")
with torch.no_grad():
    logits = model(**inputs).logits
    #probs = logits.int().numpy()[0]
    print('Prediction:')
    log_probs = get_probs(logits)
    print('--------------------')

target_list = ["Playful", "Infuriating", "Sentimental", "Cynical", "Depressing", "Awe-inspiring", "Patriotic", "Begrudging", "Educational", "Hopeful",
"Sarcastic", "Disrespectful", "Disparaging"]

final_log_probs = []
for log in log_probs:
  final_log_probs.append(log.numpy())

final_output = []
for i in zip(final_log_probs[0], target_list):
  final_output.append(i)

final_output.sort(reverse=True)
final_output

#connecting to my hugging face account
#hide_output
from huggingface_hub import notebook_login

notebook_login()

#loading the model and tokenizer to valurank hugging face space
model.push_to_hub('finetuned-distilbert-multi-label-emotion_headline_2')

model.eval()
model.to("cuda")

print('Testing started...')
print()

total_test_accuracy = 0
total_test_f1_micro = 0
total_test_f1_macro = 0

for batch in test_dataloader:
    batch_input_ids = batch[0].to(device)
    batch_input_mask = batch[1].to(device)
    batch_labels = batch[2].float().to(device)

    with torch.no_grad():
        outputs = model(batch_input_ids,
                        attention_mask=batch_input_mask,
                        labels=batch_labels)
    logits = outputs.logits
    logits = logits.detach().cpu().numpy()
    label_ids = batch_labels.to('cpu').numpy()

    total_test_f1_micro += compute_f1_micro(get_probs(logits), label_ids)
    total_test_f1_macro += compute_f1_macro(get_probs(logits), label_ids)
    total_test_accuracy += flat_accuracy(get_probs(logits), label_ids)


avg_test_accuracy = total_test_accuracy / len(test_dataloader)
avg_test_f1_micro = total_test_f1_micro / len(test_dataloader)
avg_test_f1_macro = total_test_f1_macro / len(test_dataloader)

print('Test metrics:')
print('----------------------')
print(f'Accuracy : {avg_test_accuracy:.4f}')
print(f'f1-score micro : {avg_test_f1_micro:.4f}')
print(f'f1-score macro : {avg_test_f1_macro:.4f}')

print()
print("Testing finished...")

